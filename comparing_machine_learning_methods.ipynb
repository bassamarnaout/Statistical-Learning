{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"Comparing Machine Learning Methods.ipynb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Automatically generated by Colaboratory."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Original file is located at\n", "    https://colab.research.google.com/drive/11VDNoEPj8KgT3KxssfDtcsZpmUKlv9YF"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Refernce:\n", "https://pythondata.com/category/machine-learning/"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialization"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "from google.colab import drive<br>\n", "drive.mount('/content/drive')<br>\n", "import os<br>\n", "os.chdir(\"/content/drive/My Drive/ComparingMachineLearningMethods/\")<br>\n", "!ls<br>\n", "his is a tutorial on how to compare machine learning methods with the python library scikit-learn. We'll be using the Indian Liver Disease dataset (found here https://www.kaggle.com/uciml/indian-liver-patient-records)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["From the dataset page:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"This data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh, India. The \"Dataset\" column is a class label used to divide groups into liver patient (liver disease) or not (no disease). This data set contains 441 male patient records and 142 female patient records.\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["I've used Jason Brownlee's article (https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/) from 2016 as the basis for this article...I wanted to expand a bit on what he did as well as use a different dataset.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import matplotlib.pyplot as plt\n", "plt.rcParams[\"figure.figsize\"] = (20,10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pandas.plotting import scatter_matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import model_selection\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Load Machine Learning Data\n<br>\n", "#read in the data<br>\n", "data = pd.read_csv('indian_liver_patient.csv')<br>\n", "print(data.shape)<br>\n", "# #read in the data<br>\n", "# data = pd.read_csv('HCV-Egy-Data.csv')<br>\n", " Understand Your Data With Descriptive Statistics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["You must understand your data in order to get the best results. We will discover\n", "7 recipes that you can use in Python to better understand your machine learning data. After\n", "reading this lesson you will know how to:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["1. Take a peek at your raw data.\n", "2. Review the dimensions of your dataset.\n", "3. Review the data types of attributes in your data.\n", "4. Summarize the distribution of instances across classes in your dataset.\n", "5. Summarize your data using descriptive statistics.\n", "6. Understand the relationships in your data using correlations.\n", "7. Review the skew of the distributions of each attribute.\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": [". Peek at Your Data<br>\n", "eview the first 20 rows"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data.head(20)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["peek = data.head(20)\n", "print(peek)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nWe'll use all columns except Gender for this tutorial. We could use gender by converting the gender to a numeric value (e.g., 0 for Male, 1 for Female) but for the purproses of this post, we'll just skip this column.\n<br>\n", "data_to_use = data<br>\n", "del data_to_use['Gender']<br>\n", "data_to_use.dropna(inplace=True)<br>\n", "# print(data_to_use.shape)<br>\n", "# data_to_use = data<br>\n", "# # del data_to_use['Gender']<br>\n", "# # data_to_use.dropna(inplace=True)<br>\n", "peek = data_to_use.head(20)<br>\n", "print(peek)<br>\n", "he 'Dataset' column is the value we are trying to predict...whether the user has liver disease or not so we'll that as our \"Y\" and the other columns for our \"X\" array.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [".Dimensions of Your Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(data_to_use.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": [".Data Type For Each Attribute"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["types = data_to_use.dtypes\n", "print(types)"]}, {"cell_type": "markdown", "metadata": {}, "source": [".Descriptive Statistics<br>\n", "Descriptive statistics can give you great insight into the shape of each attribute. Often you can<br>\n", "create more summaries than you have time to review. The describe() function on the Pandas<br>\n", "DataFrame lists 8 statistical properties of each attribute. They are:<br>\n", "\u0088 Count.<br>\n", "\u0088 Mean.<br>\n", "\u0088 Standard Deviation.<br>\n", "\u0088 Minimum Value.<br>\n", "\u0088 25th Percentile.<br>\n", "\u0088 50th Percentile (Median).<br>\n", "\u0088 75th Percentile.<br>\n", "\u0088 Maximum Value."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pd.set_option('display.width', 100)\n", "pd.set_option('precision', 3)\n", "description = data.describe()\n", "print(description)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["5.Class Distribution (Classi\fcation Only)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class_counts = data_to_use.groupby('Dataset').size()\n", "print(class_counts)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["6.Correlations Between Attributes<br>\n", "Correlation refers to the relationship between two variables and how they may or may not<br>\n", "change together. The most common method for calculating correlation is Pearson's Correlation<br>\n", "Coe\u000ecient, that assumes a normal distribution of the attributes involved. A correlation of -1<br>\n", "or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no<br>\n", "correlation at all. Some machine learning algorithms like linear and logistic regression can su\u000ber<br>\n", "poor performance if there are highly correlated attributes in your dataset. As such, it is a good<br>\n", "idea to review all of the pairwise correlations of the attributes in your dataset. You can use the<br>\n", "corr() function on the Pandas DataFrame to calculate a correlation matrix."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Pairwise Pearson correlations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["correlations = data_to_use.corr(method='pearson')\n", "print(correlations)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["7.Skew of Univariate Distributions<br>\n", "Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or<br>\n", "squashed in one direction or another. Many machine learning algorithms assume a Gaussian<br>\n", "distribution. Knowing that an attribute has a skew may allow you to perform data preparation<br>\n", "to correct the skew and later improve the accuracy of your models. You can calculate the skew<br>\n", "of each attribute using the skew() function on the Pandas DataFrame."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Skew for each attribute"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["skew = data.skew()\n", "print(skew)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Understand Your Data With Visualization<br>\n", "You must understand your data in order to get the best results from machine learning algorithms.<br>\n", "The fastest way to learn more about your data is to use data visualization.<br>\n", "plot your data using:<br>\n", "**Univariate Plots**<br>\n", "1.   Histograms.<br>\n", "2.   Density Plots.<br>\n", "3.   Box and Whisker Plots.<br>\n", "**Multivariate Plots**<br>\n", "4.   Correlation Matrix Plot.<br>\n", "5.   Scatter Plot Matrix.<br>\n", "Univariate Plots<br>\n", "# (1)Univariate Plots<br>\n", "1.   Histograms.<br>\n", "2.   Density Plots.<br>\n", "3.   Box and Whisker Plots.<br>\n", "**Histograms**<br>\n", "A fast way to get an idea of the distribution of each attribute is to look at histograms. Histograms<br>\n", "group data into bins and provide you a count of the number of observations in each bin. From<br>\n", "the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed<br>\n", "or even has an exponential distribution. It can also help you see possible outliers.*italicized text*<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Univariate Histograms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_to_use.hist()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can see that perhaps the attributes Alkaline_Phosphotase has an exponential<br>\n", "distribution. We can also see that perhaps the Age and Albumin and Albumin_and_Globulin_Ratio and<br>\n", "Total_Protiens attributes may have a<br>\n", "Gaussian or nearly Gaussian distribution. This is interesting because many machine learning<br>\n", "techniques assume a Gaussian univariate distribution on the input variables."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n**Density Plots**<br>\n", "Density plots are another way of getting a quick idea of the distribution of each attribute. The<br>\n", "plots look like an abstracted histogram with a smooth curve drawn through the top of each bin,<br>\n", "much like your eye tried to do with the histograms.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Univariate Density Plots"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_to_use.plot(kind='density', subplots=True, layout=(3,4), sharex=False)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n**Box and Whisker Plots**<br>\n", "Another useful way to review the distribution of each attribute is to use Box and Whisker Plots<br>\n", "or boxplots for short. Boxplots summarize the distribution of each attribute, drawing a line for<br>\n", "the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of<br>\n", "the data). The whiskers give an idea of the spread of the data and dots outside of the whiskers<br>\n", "show candidate outlier values (values that are 1.5 times greater than the size of spread of the<br>\n", "middle 50% of the data).<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Box and Whisker Plots"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data.plot(kind='box', subplots=True, layout=(3,4), sharex=False, sharey=False)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# (2)Multivariate Plots<br>\n", "This section provides examples of two plots that show the interactions between multiple variables<br>\n", "in your dataset.<br>\n", "1.   Correlation Matrix Plot.<br>\n", "2.   Scatter Plot Matrix.<br>\n", "**Correlation Matrix Plot**<br>\n", "Correlation gives an indication of how related the changes are between two variables. If two<br>\n", "variables change in the same direction they are positively correlated. If they change in opposite<br>\n", "directions together (one goes up, one goes down), then they are negatively correlated. You can<br>\n", "calculate the correlation between each pair of attributes. This is called a correlation matrix. You<br>\n", "can then plot the correlation matrix and get an idea of which variables have a high correlation with each other. This is useful to know, because some machine learning algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in your data.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Correction Matrix Plot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["correlations = data_to_use.corr()\n", "# plot correlation matrix\n", "fig = plt.figure()\n", "ax = fig.add_subplot(111)\n", "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n", "fig.colorbar(cax)\n", "ticks = np.arange(0,9,1)\n", "ax.set_xticks(ticks)\n", "ax.set_yticks(ticks)\n", "names = list(data_to_use.columns)\n", "ax.set_xticklabels(names)\n", "ax.set_yticklabels(names)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n**Scatter Plot Matrix**<br>\n", "A scatter plot shows the relationship between two variables as dots in two dimensions, one<br>\n", "axis for each attribute. You can create a scatter plot for each pair of attributes in your data.<br>\n", "Drawing all these scatter plots together is called a scatter plot matrix. Scatter plots are useful<br>\n", "for spotting structured relationships between variables, like whether you could summarize the<br>\n", "relationship between two variables with a line. Attributes with structured relationships may<br>\n", "also be correlated and good candidates for removal from your dataset.<br>\n", "Like the Correlation Matrix Plot above, the scatter plot matrix is symmetrical. This is<br>\n", "useful to look at the pairwise relationships from di\u000berent perspectives. Because there is little<br>\n", "point of drawing a scatter plot of each variable with itself, the diagonal shows histograms of<br>\n", "each attribute.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scatterplot Matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scatter_matrix(data_to_use)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Prepare Your Data For Machine Learning<br>\n", "Many machine learning algorithms make assumptions about your data. It is often a very good<br>\n", "idea to prepare your data in such way to best expose the structure of the problem to the machine<br>\n", "learning algorithms that you intend to use. In this chapter you will discover how to prepare<br>\n", "your data for machine learning in Python using scikit-learn. After completing this lesson you<br>\n", "will know how to:<br>\n", "1. Rescale data.<br>\n", "2. Standardize data.<br>\n", "3. Normalize data.<br>\n", "4. Binarize data.<br>\n", "Let's get started.<br>\n", "**Need For Data Pre-processing**<br>\n", "You almost always need to pre-process your data. It is a required step. A di\u000eculty is that<br>\n", "di\u000berent algorithms make di\u000berent assumptions about your data and may require di\u000berent<br>\n", "transforms. Further, when you follow all of the rules and prepare your data, sometimes algorithms<br>\n", "can deliver better results without pre-processing.<br>\n", "Generally, I would recommend creating many di\u000berent views and transforms of your data,<br>\n", "then exercise a handful of algorithms on each view of your dataset. This will help you to <br>\n", "ush<br>\n", "out which data transforms might be better at exposing the structure of your problem in general.<br>\n", "**Data Transforms**<br>\n", "In this lesson you will work through 4 di\u000berent data pre-processing recipes for machine learning.<br>\n", "The Pima Indian diabetes dataset is used in each recipe. Each recipe follows the same structure:<br>\n", "\u0088 <br>\n", "1.   Load the dataset from a URL.<br>\n", "2.   Split the dataset into the input and output variables for machine learning.<br>\n", "3. Apply a pre-processing transform to the input variables.<br>\n", "4. Summarize the data to show the change.<br>\n", "The scikit-learn library provides two standard idioms for transforming data. Each are useful<br>\n", "in di\u000berent circumstances. The transforms are calculated in such a way that they can be applied<br>\n", "to your training data and any samples of data you may have in the future. <br>\n", "The scikit-learn documentation has some information on how to use various di\u000berent pre-processing methods:<br>\n", "1.   \u0088 Fit and Multiple Transform.<br>\n", "2.   Combined Fit-And-Transform.<br>\n", "The Fit and Multiple Transform method is the preferred approach. You call the fit()<br>\n", "function to prepare the parameters of the transform once on your data. Then later you can use<br>\n", "the transform() function on the same data to prepare it for modeling and again on the test or<br>\n", "validation dataset or new data that you may see in the future. The Combined Fit-And-Transform<br>\n", "is a convenience that you can use for one o\u000b tasks. This might be useful if you are interested<br>\n", "in plotting or summarizing the transformed data.<br>\n", "# 1.Rescale Data<br>\n", "When your data is comprised of attributes with varying scales, many machine learning algorithms<br>\n", "can bene\ft from rescaling the attributes to all have the same scale. Often this is referred to<br>\n", "as normalization and attributes are often rescaled into the range between 0 and 1. This is<br>\n", "useful for optimization algorithms used in the core of machine learning algorithms like gradient<br>\n", "descent. It is also useful for algorithms that weight inputs like regression and neural networks<br>\n", "and algorithms that use distance measures like k-Nearest Neighbors. You can rescale your data<br>\n", "using scikit-learn using the MinMaxScaler class<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Rescale data (between 0 and 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\n", "from numpy import set_printoptions\n", "# dataframe = read_csv(filename, names=names)\n", "array = data_to_use.values\n", "# print(array)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["separate array into input and output components"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = array[:,0:9]\n", "# print(X)\n", "Y = array[:,9]\n", "# print(Y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = MinMaxScaler(feature_range=(0, 1))\n", "rescaledX = scaler.fit_transform(X)\n", "# summarize transformed data\n", "pd.set_option('precision', 3)\n", "set_printoptions(precision=3)\n", "print(rescaledX[0:5,:])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 2.Standardize Data<br>\n", "Standardization is a useful technique to transform attributes with a Gaussian distribution and<br>\n", "# di\u000bffering means and standard deviations to a standard Gaussian distribution with a mean of<br>\n", "0 and a standard deviation of 1. It is most suitable for techniques that assume a Gaussian<br>\n", "distribution in the input variables and work better with rescaled data, such as linear regression,<br>\n", "logistic regression and linear discriminate analysis. You can standardize data using scikit-learn<br>\n", "with the StandardScaler class<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Standardize data (0 mean, 1 stdev)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n", "from numpy import set_printoptions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["array = data_to_use.values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["separate array into input and output components"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = array[:,0:9]\n", "# print(X)\n", "Y = array[:,9]\n", "# print(Y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler().fit(X)\n", "rescaledX = scaler.transform(X)\n", "# summarize transformed data\n", "set_printoptions(precision=3)\n", "print(rescaledX[0:5,:])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 3.Normalize Data<br>\n", "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called<br>\n", "a unit norm or a vector with the length of 1 in linear algebra). This pre-processing method<br>\n", "can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using<br>\n", "algorithms that weight input values such as neural networks and algorithms that use distance<br>\n", "measures such as k-Nearest Neighbors. You can normalize data in Python with scikit-learn<br>\n", "using the Normalizer class<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Normalize data (length of 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import Normalizer\n", "from numpy import set_printoptions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["array = data_to_use.values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["separate array into input and output components"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = array[:,0:9]\n", "Y = array[:,9]\n", "scaler = Normalizer().fit(X)\n", "normalizedX = scaler.transform(X)\n", "# summarize transformed data\n", "set_printoptions(precision=3)\n", "print(normalizedX[0:5,:])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 4.Binarize Data (Make Binary)<br>\n", "You can transform your data using a binary threshold. All values above the threshold are<br>\n", "marked 1 and all equal to or below are marked as 0. This is called binarizing your data or<br>\n", "thresholding your data. It can be useful when you have probabilities that you want to make crisp<br>\n", "values. It is also useful when feature engineering and you want to add new features that indicate<br>\n", "something meaningful. You can create new binary attributes in Python using scikit-learn with<br>\n", "the Binarizer class<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["binarization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import Binarizer\n", "from numpy import set_printoptions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["array = data_to_use.values\n", "print(array)\n", "# separate array into input and output components\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "binarizer = Binarizer(threshold=30.0).fit(X)\n", "binaryX = binarizer.transform(X)\n", "# summarize transformed data\n", "set_printoptions(precision=3)\n", "print(binaryX[0:5,:])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Feature Selection For Machine Learning<br>\n", "The data features that you use to train your machine learning models have a huge influence on the performance you can achieve. Irrelevant or partially relevant features can negatively impact model performance. In this chapter you will discover automatic feature selection techniques<br>\n", "that you can use to prepare your machine learning data in Python with scikit-learn. After completing this lesson you will know how to use:<br>\n", "1. Univariate Selection.<br>\n", "2. Recursive Feature Elimination.<br>\n", "3. Principle Component Analysis.<br>\n", "4. Feature Importance.<br>\n", "# 1.Univariate Selection<br>\n", "Statistical tests can be used to select those features that have the strongest relationship with<br>\n", "the output variable. The scikit-learn library provides the SelectKBest class2 that can be used<br>\n", "with a suite of diff\u000berent statistical tests to select a specifi\fc number of features. The example<br>\n", "below uses the chi-squared (chi2) statistical test for non-negative features to select 4 of the best<br>\n", "features from the dataset.<br>\n", "You can see the scores for each attribute and the 4 attributes chosen (those with the highest<br>\n", "scores): plas, test, mass and age. I got the names for the chosen attributes by manually<br>\n", "mapping the index of the 4 highest scores to the index of the attribute names.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from numpy import set_printoptions\n", "from sklearn.feature_selection import SelectKBest\n", "from sklearn.feature_selection import chi2\n", "array = data_to_use.values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = array[:,0:9]\n", "Y = array[:,9]\n", "# feature extraction\n", "test = SelectKBest(score_func=chi2, k=4)\n", "fit = test.fit(X, Y)\n", "# summarize scores\n", "set_printoptions(precision=3)\n", "print(fit.scores_)\n", "features = fit.transform(X)\n", "# summarize selected features\n", "print(features[0:5,:])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 2.Recursive Feature Elimination<br>\n", "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and<br>\n", "building a model on those attributes that remain. It uses the model accuracy to identify which<br>\n", "attributes (and combination of attributes) contribute the most to predicting the target attribute.<br>\n", "You can learn more about the RFE class3 in the scikit-learn documentation. The example below<br>\n", "uses RFE with the logistic regression algorithm to select the top 3 features. The choice of<br>\n", "algorithm does not matter too much as long as it is skillful and consistent.<br>\n", "You can see that RFE chose the top 3 features as preg, mass and pedi. These are marked<br>\n", "True in the support array and marked with a choice 1 in the ranking array. Again, you can<br>\n", "manually map the feature indexes to the indexes of attribute names.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Feature Extraction with RFE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_selection import RFE\n", "from sklearn.linear_model import LogisticRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "# feature extraction\n", "model = LogisticRegression()\n", "rfe = RFE(model, 3)\n", "fit = rfe.fit(X, Y)\n", "# print(\"Num Features: %d\") %fit.n_features_\n", "print(\"Num Features:\")\n", "print(fit.n_features_)\n", "# print(\"Selected Features: %s\") % fit.support_\n", "print(\"Selected Features:\")\n", "print(fit.support_)\n", "# print(\"Feature Ranking: %s\") % fit.ranking_\n", "print(\"Feature Ranking:\")\n", "print(fit.ranking_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 3.Principal Component Analysis<br>\n", "Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a<br>\n", "compressed form. Generally this is called a data reduction technique. A property of PCA is that<br>\n", "you can choose the number of dimensions or principal components in the transformed result. In<br>\n", "the example below, we use PCA and select 3 principal components.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Feature Extraction with PCA"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.decomposition import PCA\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "# feature extraction\n", "pca = PCA(n_components=3)\n", "fit = pca.fit(X)\n", "# summarize components\n", "# print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n", "print(\"Explained Variance:\")\n", "print(fit.explained_variance_ratio_)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(fit.components_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 4.Feature Importance<br>\n", "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance<br>\n", "of features. In the example below we construct a ExtraTreesClassifier classi\fer for dataset.<br>\n", "You can see that we are given an importance score for each attribute where the larger the<br>\n", "score, the more important the attribute. The scores suggest at the importance of plas, age<br>\n", "and mass.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Feature Importance with Extra Trees Classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import ExtraTreesClassifier\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "# feature extraction\n", "model = ExtraTreesClassifier()\n", "model.fit(X, Y)\n", "print(model.feature_importances_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Evaluate the Performance of Machine Learning Algorithms with Resampling<br>\n", "You need to know how well your algorithms perform on unseen data. The best way to evaluate<br>\n", "the performance of an algorithm would be to make predictions for new data to which you<br>\n", "already know the answers. The second best way is to use clever techniques from statistics called<br>\n", "resampling methods that allow you to make accurate estimates for how well your algorithm will<br>\n", "perform on new data. In this chapter you will discover how you can estimate the accuracy of<br>\n", "your machine learning algorithms using resampling methods in Python and scikit-learn on the dataset.<br>\n", "The evaluation is an estimate that we can use to talk about how well we think the algorithm<br>\n", "may actually do in practice. It is not a guarantee of performance. Once we estimate the<br>\n", "performance of our algorithm, we can then re-train the \ffinal algorithm on the entire training<br>\n", "dataset and get it ready for operational use. Next up we are going to look at four di\u000berent<br>\n", "techniques that we can use to split up our training dataset and create useful estimates of<br>\n", "performance for our machine learning algorithms:<br>\n", "\u0088 <br>\n", "1.   Train and Test Sets.<br>\n", "2.   k-fold Cross Validation.<br>\n", "3.   Leave One Out Cross Validation.<br>\n", "4.   Repeated Random Test-Train Splits.<br>\n", "\u0088 <br>\n", "\u0088 <br>\n", "\u0088<br>\n", "# 1.Split into Train and Test Sets<br>\n", "The simplest method that we can use to evaluate the performance of a machine learning<br>\n", "algorithm is to use di\u000berent training and testing datasets. We can take our original dataset and<br>\n", "split it into two parts. Train the algorithm on the fi\frst part, make predictions on the second<br>\n", "part and evaluate the predictions against the expected results. The size of the split can depend<br>\n", "on the size and specifi\fcs of your dataset, although it is common to use 67% of the data for<br>\n", "training and the remaining 33% for testing.<br>\n", "This algorithm evaluation technique is very fast. It is ideal for large datasets (millions of<br>\n", "records) where there is strong evidence that both splits of the data are representative of the<br>\n", "underlying problem. Because of the speed, it is useful to use this approach when the algorithm<br>\n", "you are investigating is slow to train. A downside of this technique is that it can have a high<br>\n", "variance. This means that di\u000bfferences in the training and test dataset can result in meaningful<br>\n", "diff\u000berences in the estimate of accuracy. In the example below we split the dataset<br>\n", "into 67%/33% splits for training and test and evaluate the accuracy of a Logistic Regression<br>\n", "model.<br>\n", "We can see that the estimated accuracy for the model was approximately 75%. Note that<br>\n", "in addition to specifying the size of the split, we also specify the random seed. Because the<br>\n", "split of the data is random, we want to ensure that the results are reproducible. By specifying<br>\n", "the random seed we ensure that we get the same random numbers each time we run the code<br>\n", "and in turn the same split of data. This is important if we want to compare this result to<br>\n", "the estimated accuracy of another machine learning algorithm or the same algorithm with a<br>\n", "di\u000bfferent con\ffiguration. To ensure the comparison was apples-for-apples, we must ensure that<br>\n", "they are trained and tested on exactly the same data.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate using a train and a test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "test_size = 0.33\n", "seed = 7\n", "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n", "random_state=seed)\n", "model = LogisticRegression()\n", "model.fit(X_train, Y_train)\n", "result = model.score(X_test, Y_test)\n", "# print('Accuracy: %.3f%%') % (result*100.0)\n", "print('Accuracy:')\n", "print(result*100.0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 2.K-fold Cross Validation<br>\n", "Cross validation is an approach that you can use to estimate the performance of a machine<br>\n", "learning algorithm with less variance than a single train-test set split. It works by splitting<br>\n", "the dataset into k-parts (e.g. k = 5 or k = 10). Each split of the data is called a fold. The<br>\n", "algorithm is trained on k - 1 folds with one held back and tested on the held back fold. This is<br>\n", "repeated so that each fold of the dataset is given a chance to be the held back test set. After<br>\n", "running cross validation you end up with k diff\u000berent performance scores that you can summarize<br>\n", "using a mean and a standard deviation.<br>\n", "The result is a more reliable estimate of the performance of the algorithm on new data. It is<br>\n", "more accurate because the algorithm is trained and evaluated multiple times on diff\u000berent data.<br>\n", "The choice of k must allow the size of each test partition to be large enough to be a reasonable<br>\n", "sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the<br>\n", "algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest<br>\n", "sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are<br>\n", "common. In the example below we use 10-fold cross validation.<br>\n", "You can see that we report both the mean and the standard deviation of the performance<br>\n", "measure. When summarizing performance measures, it is a good practice to summarize the<br>\n", "distribution of the measures, in this case assuming a Gaussian distribution of performance (a<br>\n", "very reasonable assumption) and recording the mean and standard deviation.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate using Cross Validation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LogisticRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "num_folds = 10\n", "seed = 7\n", "kfold = KFold(n_splits=num_folds, random_state=seed)\n", "model = LogisticRegression()\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "# print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)\n", "print(\"Accuracy:\")\n", "print(results.mean()*100.0)\n", "print(results.std()*100.0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 3.Leave One Out Cross Validation<br>\n", "You can confi\fgure cross validation so that the size of the fold is 1 (k is set to the number of<br>\n", "observations in your dataset). This variation of cross validation is called leave-one-out cross<br>\n", "validation. The result is a large number of performance measures that can be summarized in an eff\u000bort to give a more reasonable estimate of the accuracy of your model on unseen data.<br>\n", "A downside is that it can be a computationally more expensive procedure than k-fold cross<br>\n", "validation. In the example below we use leave-one-out cross validation.<br>\n", "You can see in the standard deviation that the score has more variance than the k-fold cross<br>\n", "validation results described above.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate using Leave One Out Cross Validation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import LeaveOneOut\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LogisticRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "num_folds = 10\n", "loocv = LeaveOneOut()\n", "model = LogisticRegression(solver = 'lbfgs')\n", "results = cross_val_score(model, X, Y, cv=loocv)\n", "# print(\"Accuracy: /%.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)\n", "print(\"Accuracy:\")\n", "print(results.mean()*100.0)\n", "print(results.std()*100.0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 4.Repeated Random Test-Train Splits<br>\n", "Another variation on k-fold cross validation is to create a random split of the data like the<br>\n", "train/test split described above, but repeat the process of splitting and evaluation of the<br>\n", "algorithm multiple times, like cross validation. This has the speed of using a train/test split and<br>\n", "the reduction in variance in the estimated performance of k-fold cross validation. You can also<br>\n", "repeat the process many more times as needed to improve the accuracy. A down side is that<br>\n", "repetitions may include much of the same data in the train or the test split from run to run,<br>\n", "introducing redundancy into the evaluation. The example below splits the data into a 67%/33%<br>\n", "train/test split and repeats the process 10 times.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate using Shuffle Split Cross Validation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import ShuffleSplit\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LogisticRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "n_splits = 10\n", "test_size = 0.33\n", "seed = 7\n", "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n", "model = LogisticRegression()\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "# print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)\n", "print(\"Accuracy:\")\n", "print(results.mean()*100.0)\n", "print(results.std()*100.0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# What Techniques to Use When<br>\n", "This section lists some tips to consider what resampling technique to use in di\u000bfferent circum-stances.<br>\n", "*   Generally k-fold cross validation is the gold standard for evaluating the performance of a<br>\n", "machine learning algorithm on unseen data with k set to 3, 5, or 10.<br>\n", "*   Using a train/test split is good for speed when using a slow algorithm and produces<br>\n", "performance estimates with lower bias when using large datasets.<br>\n", "*   Techniques like leave-one-out cross validation and repeated random splits can be useful<br>\n", "intermediates when trying to balance variance in the estimated performance, model<br>\n", "training speed and dataset size.<br>\n", "The best advice is to experiment and \ffind a technique for your problem that is fast and<br>\n", "produces reasonable estimates of performance that you can use to make decisions. If in doubt,<br>\n", "use 10-fold cross validation.<br>\n", "# Machine Learning Algorithm Performance Metrics<br>\n", "The metrics that you choose to evaluate your machine learning algorithms are very important.<br>\n", "Choice of metrics influences how the performance of machine learning algorithms is measured<br>\n", "and compared. They influence how you weight the importance of diff\u000berent characteristics in<br>\n", "the results and your ultimate choice of which algorithm to choose. In this chapter you will<br>\n", "discover how to select and use diff\u000berent machine learning performance metrics in Python with<br>\n", "scikit-learn. Let's get started.<br>\n", "**Algorithm Evaluation Metrics**<br>\n", "various diff\u000berent algorithm evaluation metrics are demonstrated for both classi\fffication and regression type machine learning problems.<br>\n", "All recipes evaluate the same algorithms, Logistic Regression for classi\fcation and Linear<br>\n", "Regression for the regression problems. A 10-fold cross validation test harness is used to<br>\n", "demonstrate each metric, because this is the most likely scenario you will use when employing<br>\n", "di\u000bfferent algorithm evaluation metrics.<br>\n", "**Classifi\fcation Metrics**<br>\n", "Classi\fcation problems are perhaps the most common type of machine learning problem and as<br>\n", "such there are a myriad of metrics that can be used to evaluate predictions for these problems.<br>\n", "In this section we will review how to use the following metrics:<br>\n", "*   Classifi\fcation Accuracy.<br>\n", "*   Logarithmic Loss.<br>\n", "*   Area Under ROC Curve.<br>\n", "*   Confusion Matrix.<br>\n", "*   Classi\fcation Report.<br>\n", "**Regression Metrics**<br>\n", "In this section will review 3 of the most common metrics for evaluating predictions on regression<br>\n", "machine learning problems:<br>\n", "*   Mean Absolute Error.<br>\n", "*   Mean Squared Error.<br>\n", "*   R2.<br>\n", "# 1.Classifi\fcation Accuracy<br>\n", "Classifi\fcation accuracy is the number of correct predictions made as a ratio of all predictions<br>\n", "made. This is the most common evaluation metric for classi\fcation problems, it is also the most<br>\n", "misused. It is really only suitable when there are an equal number of observations in each class<br>\n", "(which is rarely the case) and that all predictions and prediction errors are equally important,<br>\n", "which is often not the case. Below is an example of calculating classifi\fcation accuracy.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross Validation Classification Accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LogisticRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = LogisticRegression()\n", "scoring = 'accuracy'\n", "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n", "# print(\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std())\n", "print(\"Accuracy:\")\n", "print(results.mean())\n", "print(results.std())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can see that the ratio is reported. This can be converted into a percentage by multiplying<br>\n", "the value by 100, giving an accuracy score of approximately 77% accurate."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 2.Logarithmic Loss<br>\n", "Logarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities<br>\n", "of membership to a given class. The scalar probability between 0 and 1 can be seen as a measure<br>\n", "of con\ffidence for a prediction by an algorithm. Predictions that are correct or incorrect are<br>\n", "rewarded or punished proportionally to the con\ffidence of the prediction. Below is an example<br>\n", "of calculating logloss for Logistic regression predictions on the dataset.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross Validation Classification LogLoss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LogisticRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = LogisticRegression()\n", "scoring = 'neg_log_loss'\n", "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n", "# print(\"Logloss: %.3f (%.3f)\") % (results.mean(), results.std())\n", "print(\"Logloss:\")\n", "print(results.mean())\n", "print(results.std())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Smaller logloss is better with 0 representing a perfect logloss. As mentioned above, the<br>\n", "measure is inverted to be ascending when using the cross val score() function."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 3.Area Under ROC Curve<br>\n", "Area under ROC Curve (or AUC for short) is a performance metric for binary classi\fcation<br>\n", "problems. The AUC represents a model's ability to discriminate between positive and negative<br>\n", "classes. An area of 1.0 represents a model that made all predictions perfectly. An area of<br>\n", "0.5 represents a model that is as good as random. ROC can be broken down into sensitivity<br>\n", "and speci\fcity. A binary classi\fcation problem is really a trade-o\u000bff between sensitivity and<br>\n", "speci\fficity.<br>\n", "*   Sensitivity is the true positive rate also called the recall. It is the number of instances<br>\n", "from the positive (\frst) class that actually predicted correctly<br>\n", "*   Speci\fcity is also called the true negative rate. Is the number of instances from the<br>\n", "negative (second) class that were actually predicted correctly.<br>\n", "\u0088<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross Validation Classification ROC AUC"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LogisticRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = LogisticRegression()\n", "scoring = 'roc_auc'\n", "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n", "# print(\"AUC: %.3f (%.3f)\") % (results.mean(), results.std())\n", "print(\"AUC:\")\n", "print(results.mean())\n", "print(results.std())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can see the AUC is relatively close to 1 and greater than 0.5, suggesting some skill in<br>\n", "the predictions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 4.Confusion Matrix<br>\n", "The confusion matrix is a handy presentation of the accuracy of a model with two or more<br>\n", "classes. The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The<br>\n", "cells of the table are the number of predictions made by a machine learning algorithm. For<br>\n", "example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have<br>\n", "been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction = 0 and<br>\n", "actual = 0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0<br>\n", "and actual = 1. And so on. Below is an example of calculating a confusion matrix for a set of<br>\n", "predictions by a Logistic Regression on the Pima Indians onset of diabetes dataset.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross Validation Classification Confusion Matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import confusion_matrix\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "test_size = 0.33\n", "seed = 7\n", "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n", "random_state=seed)\n", "model = LogisticRegression()\n", "model.fit(X_train, Y_train)\n", "predicted = model.predict(X_test)\n", "matrix = confusion_matrix(Y_test, predicted)\n", "print(matrix)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Although the array is printed without headings, you can see that the majority of the<br>\n", "predictions fall on the diagonal line of the matrix (which are correct predictions)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 5.Classi\fcation Report<br>\n", "The scikit-learn library provides a convenience report when working on classi\fcation prob-lems to give you a quick idea of the accuracy of a model using a number of measures. The<br>\n", "classification report() function displays the precision, recall, F1-score and support for each<br>\n", "class. The example below demonstrates the report on the binary classi\fcation problem.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross Validation Classification Report"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import classification_report\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "test_size = 0.33\n", "seed = 7\n", "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n", "random_state=seed)\n", "model = LogisticRegression()\n", "model.fit(X_train, Y_train)\n", "predicted = model.predict(X_test)\n", "report = classification_report(Y_test, predicted)\n", "print(report)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n#**Regression Metrics**<br>\n", "In this section will review 3 of the most common metrics for evaluating predictions on regression<br>\n", "machine learning problems:<br>\n", "*   Mean Absolute Error.<br>\n", "*   Mean Squared Error.<br>\n", "*   R2.<br>\n", "# 1.Mean Absolute Error<br>\n", "The Mean Absolute Error (or MAE) is the sum of the absolute diff\u000berences between predictions<br>\n", "and actual values. It gives an idea of how wrong the predictions were. The measure gives an<br>\n", "idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).<br>\n", "The example below demonstrates calculating mean absolute error on the Boston house price<br>\n", "dataset.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross Validation Regression MAE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LinearRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = LinearRegression()\n", "scoring = 'neg_mean_absolute_error'\n", "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n", "# print(\"MAE: %.3f (%.3f)\") % (results.mean(), results.std())\n", "print(\"MAE:\")\n", "print(results.mean())\n", "print(results.std())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A value of 0 indicates no error or perfect predictions. Like logloss, this metric is inverted by<br>\n", "the cross val score() function."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 2.Mean Squared Error<br>\n", "The Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a<br>\n", "gross idea of the magnitude of error. Taking the square root of the mean squared error converts<br>\n", "the units back to the original units of the output variable and can be meaningful for description<br>\n", "and presentation. This is called the Root Mean Squared Error (or RMSE). The example below<br>\n", "provides a demonstration of calculating mean squared error.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross Validation Regression MSE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LinearRegression"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "num_folds = 10\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = LinearRegression()\n", "scoring = 'neg_mean_squared_error'\n", "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n", "# print(\"MSE: %.3f (%.3f)\") % (results.mean(), results.std())\n", "print(\"MSE:\")\n", "print(results.mean())\n", "print(results.std())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This metric too is inverted so that the results are increasing. Remember to take the absolute<br>\n", "value before taking the square root if you are interested in calculating the RMSE."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 3.R2 Metric<br>\n", "The R2 (or R Squared) metric provides an indication of the goodness of \ffit of a set of predictions<br>\n", "to the actual values. In statistical literature this measure is called the coe\u000ecient of determination.<br>\n", "This is a value between 0 and 1 for no-\ffit and perfect \ffit respectively. The example below<br>\n", "provides a demonstration of calculating the mean R2 for a set of predictions.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross Validation Regression R^2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LinearRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = LinearRegression()\n", "scoring = 'r2'\n", "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n", "# print(\"R^2: %.3f (%.3f)\") % (results.mean(), results.std())\n", "print(\"R^2:\")\n", "print(results.mean())\n", "print(results.std())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["values = data_to_use.values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y = values[:,9]<br>\n", "X = values[:,0:9]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n**Summary**<br>\n", "In this chapter you discovered metrics that you can use to evaluate your machine learning<br>\n", "algorithms.<br>\n", "You learned about three classifi\fcation metrics: Accuracy, Logarithmic Loss and Area Under<br>\n", "ROC Curve. You also learned about two convenience methods for classi\ffication prediction<br>\n", "results: the Confusion Matrix and the Classifi\fcation Report. Finally, you also learned about<br>\n", "three metrics for regression problems: Mean Absolute Error, Mean Squared Error and R2.<br>\n", "You now know how to evaluate the performance of machine learning algorithms using a variety<br>\n", "of di\u000berent metrics and how to use those metrics to estimate the performance of algorithms on<br>\n", "new unseen data using resampling. In the next lesson you will start looking at machine learning<br>\n", "algorithms themselves, starting with classi\fcation techniques.<br>\n", "# Spot-Check Classi\fcation Algorithms<br>\n", "Spot-checking is a way of discovering which algorithms <br>\n", "perform well on your machine learning<br>\n", "problem. You cannot know which algorithms are best suited to your problem beforehand. You<br>\n", "must trial a number of methods and focus attention on those that prove themselves the most<br>\n", "promising. In this chapter you will discover six machine learning algorithms that you can use<br>\n", "when spot-checking your classi\fcation problem in Python with scikit-learn. After completing<br>\n", "this lesson you will know:<br>\n", "1. How to spot-check machine learning algorithms on a classi\fcation problem.<br>\n", "2. How to spot-check two linear classi\fcation algorithms.<br>\n", "3. How to spot-check four nonlinear classi\fcation algorithms.<br>\n", "We are going to take a look at six classi\fcation algorithms that you can spot-check on your<br>\n", "dataset. Starting with two linear machine learning algorithms:<br>\n", "\u0088<br>\n", "1.   Logistic Regression.<br>\n", "2.   Linear Discriminant Analysis.<br>\n", "Then looking at four nonlinear machine learning algorithms:<br>\n", "*   k-Nearest Neighbors.<br>\n", "*   Naive Bayes.<br>\n", "*   Classi\fcation and Regression Trees.<br>\n", "*   Support Vector Machines.<br>\n", "# Linear Machine Learning Algorithms<br>\n", "This section demonstrates minimal recipes for how to use two linear machine learning algorithms:<br>\n", "logistic regression and linear discriminant analysis.<br>\n", "# 1.Logistic Regression<br>\n", "Logistic regression assumes a Gaussian distribution for the numeric input variables and can<br>\n", "model binary classifi\fcation problems. You can construct a logistic regression model using the<br>\n", "LogisticRegression class1.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Logistic Regression Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LogisticRegression\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "num_folds = 10\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = LogisticRegression()\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean()) #Running the example prints the mean estimated accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 2.Linear Discriminant Analysis<br>\n", "Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass<br>\n", "classi\ffication. It too assumes a Gaussian distribution for the numerical input variables. You can<br>\n", "construct an LDA model using the LinearDiscriminantAnalysis class<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["LDA Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "num_folds = 10\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = LinearDiscriminantAnalysis()\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean())#Running the example prints the mean estimated accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Nonlinear Machine Learning Algorithms<br>\n", "This section demonstrates minimal recipes for how to use 4 nonlinear machine learning algorithms.<br>\n", "# 1.k-Nearest Neighbors<br>\n", "The k-Nearest Neighbors algorithm (or KNN) uses a distance metric to \fnd the k most similar<br>\n", "instances in the training data for a new instance and takes the mean outcome of the neighbors<br>\n", "as the prediction. You can construct a KNN model using the KNeighborsClassifier class<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["KNN Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.neighbors import KNeighborsClassifier\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "num_folds = 10\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = KNeighborsClassifier()\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean())#Running the example prints the mean estimated accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 2.Naive Bayes<br>\n", "Naive Bayes calculates the probability of each class and the conditional probability of each class<br>\n", "given each input value. These probabilities are estimated for new data and multiplied together,<br>\n", "assuming that they are all independent (a simple or naive assumption). When working with<br>\n", "real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for<br>\n", "input variables using the Gaussian Probability Density Function. You can construct a Naive<br>\n", "Bayes model using the GaussianNB class<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gaussian Naive Bayes Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.naive_bayes import GaussianNB\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = GaussianNB()\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean())#Running the example prints the mean estimated accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 3.Classi\fcation and Regression Trees<br>\n", "Classi\fcation and Regression Trees (CART or just decision trees) construct a binary tree from<br>\n", "the training data. Split points are chosen greedily by evaluating each attribute and each value<br>\n", "of each attribute in the training data in order to minimize a cost function (like the Gini index).<br>\n", "You can construct a CART model using the DecisionTreeClassifier class<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["CART Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.tree import DecisionTreeClassifier\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = DecisionTreeClassifier()\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean())#Running the example prints the mean estimated accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 4.Support Vector Machines<br>\n", "Support Vector Machines (or SVM) seek a line that best separates two classes. Those data<br>\n", "instances that are closest to the line that best separates the classes are called support vectors<br>\n", "and influence where the line is placed. SVM has been extended to support multiple classes.<br>\n", "Of particular importance is the use of di\u000berent kernel functions via the kernel parameter. A powerful Radial Basis Function is used by default. You can construct an SVM model using the SVC class<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["SVM Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.svm import SVC\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = SVC()\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean())#Running the example prints the mean estimated accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n**Summary**<br>\n", "In this chapter you discovered 6 machine learning algorithms that you can use to spot-check<br>\n", "on your classi\fcation problem in Python using scikit-learn. Speci\fcally, you learned how to<br>\n", "spot-check two linear machine learning algorithms: Logistic Regression and Linear Discriminant<br>\n", "Analysis. You also learned how to spot-check four nonlinear algorithms: k-Nearest Neighbors,<br>\n", "Naive Bayes, Classi\fcation and Regression Trees and Support Vector Machines.<br>\n", "# Spot-Check Regression Algorithms<br>\n", "In this lesson we are going to take a look at seven regression algorithms that you can spot-check<br>\n", "on your dataset. Starting with four linear machine learning algorithms:<br>\n", "1.   Linear Regression.<br>\n", "2.   Ridge Regression.<br>\n", "3.   LASSO Linear Regression.<br>\n", "4.   Elastic Net Regression.<br>\n", "Then looking at three nonlinear machine learning algorithms:<br>\n", "1.   k-Nearest Neighbors.<br>\n", "2.   Classi\fcation and Regression Trees.<br>\n", "3.   Support Vector Machines.<br>\n", "# A.Improve Performance with Ensembles<br>\n", "Ensembles can give you a boost in accuracy on your dataset. In this chapter you will discover<br>\n", "how you can create some of the most powerful types of ensembles in Python using scikit-learn.<br>\n", "This lesson will step you through Boosting, Bagging and Majority Voting and show you how you<br>\n", "can continue to ratchet up the accuracy of the models on your own datasets. After completing<br>\n", "this lesson you will know:<br>\n", "1. How to use bagging ensemble methods such as bagged decision trees, random forest and<br>\n", "extra trees.<br>\n", "2. How to use boosting ensemble methods such as AdaBoost and stochastic gradient boosting.<br>\n", "3. How to use voting ensemble methods to combine the predictions from multiple algorithms.<br>\n", "Let's get started.<br>\n", "**Combine Models Into Ensemble Predictions**<br>\n", "The three most popular methods for combining the predictions from di\u000berent models are:<br>\n", "\u0088 **1-Bagging**. Building multiple models (typically of the same type) from di\u000berent subsamples of the training dataset.<br>\n", "\u0088 <br>\n", " **2-Boosting**. Building multiple models (typically of the same type) each of which learns to \fx the prediction errors of a prior model in the sequence of models.<br>\n", "\u0088 **3-Voting**. Building multiple models (typically of di\u000bering types) and simple statistics (like calculating the mean) are used to combine predictions.<br>\n", "Each ensemble algorithm is demonstrated using 10-fold cross validation and the classi\fcation accuracy performance metric.<br>\n", "# A.1Bagging Algorithms<br>\n", "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset<br>\n", "(with replacement) and training a model for each sample. The \fnal output prediction is averaged<br>\n", "across the predictions of all of the sub-models. The three bagging models covered in this section<br>\n", "are as follows:<br>\n", "1.   Bagged Decision Trees.<br>\n", "2.   \u0088Random Forest.<br>\n", "3.   Extra Trees.<br>\n", "# A.1.1 Bagged Decision Trees<br>\n", "Bagging performs best with algorithms that have high variance. A popular example are<br>\n", "decision trees, often constructed without pruning. In the example below is an example<br>\n", "of using the BaggingClassifier with the Classi\fcation and Regression Trees algorithm<br>\n", "(DecisionTreeClassifier1). A total of 100 trees are created.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Bagged Decision Trees for Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.ensemble import BaggingClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "seed = 7\n", "kfold = KFold(n_splits=10, random_state=seed)\n", "cart = DecisionTreeClassifier()\n", "num_trees = 100\n", "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean())#Running the example, we get a robust estimate of model accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# A.1.2Random Forest<br>\n", "Random Forests is an extension of bagged decision trees. Samples of the training dataset are<br>\n", "taken with replacement, but the trees are constructed in a way that reduces the correlation<br>\n", "between individual classi\fers. Speci\fcally, rather than greedily choosing the best split point in<br>\n", "the construction of each tree, only a random subset of features are considered for each split. You<br>\n", "can construct a Random Forest model for classi\fcation using the RandomForestClassifier<br>\n", "class2. The example below demonstrates using Random Forest for classi\fcation with 100 trees<br>\n", "and split points chosen from a random selection of 3 features.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Random Forest Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.ensemble import RandomForestClassifier\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "num_trees = 100\n", "max_features = 3\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean()) #Running the example provides a mean estimate of classi\fcation accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# A.1.3Extra Trees<br>\n", "Extra Trees are another modi\fcation of bagging where random trees are constructed from<br>\n", "samples of the training dataset. You can construct an Extra Trees model for classi\fcation using<br>\n", "the ExtraTreesClassifier class3. The example below provides a demonstration of extra trees<br>\n", "with the number of trees set to 100 and splits chosen from 7 random features.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Extra Trees Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.ensemble import ExtraTreesClassifier\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "num_trees = 100\n", "max_features = 7\n", "kfold = KFold(n_splits=10, random_state=7)\n", "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# A.2 Boosting Algorithms<br>\n", "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes<br>\n", "of the models before them in the sequence. Once created, the models make predictions which<br>\n", "may be weighted by their demonstrated accuracy and the results are combined to create a \fnal<br>\n", "output prediction. The two most common boosting ensemble machine learning algorithms are:<br>\n", "\u0088 1.AdaBoost.<br>\n", "\u0088 2.Stochastic Gradient Boosting.<br>\n", "# A.2.1AdaBoost<br>\n", "AdaBoost was perhaps the \frst successful boosting ensemble algorithm. It generally works<br>\n", "by weighting instances in the dataset by how easy or di\u000ecult they are to classify, allowing<br>\n", "the algorithm to pay or less attention to them in the construction of subsequent models. You<br>\n", "can construct an AdaBoost model for classi\fcation using the AdaBoostClassifier class4. The<br>\n", "example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost<br>\n", "algorithm.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["AdaBoost Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.ensemble import AdaBoostClassifier\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "num_trees = 30\n", "seed=7\n", "kfold = KFold(n_splits=10, random_state=seed)\n", "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean()) #Running the example provides a mean estimate of classi\fcation accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# A.2.2Stochastic Gradient Boosting<br>\n", "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most<br>\n", "sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of<br>\n", "the best techniques available for improving performance via ensembles. You can construct a<br>\n", "Gradient Boosting model for classi\fcation using the GradientBoostingClassifier class5. The<br>\n", "example below demonstrates Stochastic Gradient Boosting for classi\fcation with 100 trees.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Stochastic Gradient Boosting Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "seed = 7\n", "num_trees = 100\n", "kfold = KFold(n_splits=10, random_state=seed)\n", "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n", "results = cross_val_score(model, X, Y, cv=kfold)\n", "print(results.mean()) #Running the example provides a mean estimate of classi\fcation accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# A.3Voting Ensemble<br>\n", "Voting is one of the simplest ways of combining the predictions from multiple machine learning<br>\n", "algorithms. It works by \frst creating two or more standalone models from your training dataset.<br>\n", "A Voting Classi\fer can then be used to wrap your models and average the predictions of the<br>\n", "sub-models when asked to make predictions for new data. The predictions of the sub-models can<br>\n", "be weighted, but specifying the weights for classi\fers manually or even heuristically is di\u000ecult.<br>\n", "More advanced methods can learn how to best weight the predictions from sub-models, but this<br>\n", "is called stacking (stacked aggregation) and is currently not provided in scikit-learn.<br>\n", "You can create a voting ensemble model for classi\fcation using the VotingClassifier<br>\n", "class6. The code below provides an example of combining the predictions of logistic regression,<br>\n", "classi\fcation and regression trees and support vector machines together for a classi\fcation<br>\n", "problem.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Voting Ensemble for Classification"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.ensemble import VotingClassifier\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "kfold = KFold(n_splits=10, random_state=7)\n", "# create the sub models\n", "estimators = []\n", "model1 = LogisticRegression()\n", "estimators.append(('logistic', model1))\n", "model2 = DecisionTreeClassifier()\n", "estimators.append(('cart', model2))\n", "model3 = SVC()\n", "estimators.append(('svm', model3))\n", "# create the ensemble model\n", "ensemble = VotingClassifier(estimators)\n", "results = cross_val_score(ensemble, X, Y, cv=kfold)\n", "print(results.mean()) #Running the example provides a mean estimate of classi\fcation accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nSummary<br>\n", "In this chapter you discovered ensemble machine learning algorithms for improving the perfor-<br>\n", "mance of models on your problems. You learned about:<br>\n", "\u0088 Bagging Ensembles including Bagged Decision Trees, Random Forest and Extra Trees.<br>\n", "\u0088 Boosting Ensembles including AdaBoost and Stochastic Gradient Boosting.<br>\n", "\u0088 Voting Ensembles for averaging the predictions for any arbitrary models.<br>\n", "# B.Improve Performance with Algorithm Tuning<br>\n", "Machine learning models are parameterized so that their behavior can be tuned for a given<br>\n", "problem. Models can have many parameters and \fnding the best combination of parameters can<br>\n", "be treated as a search problem. In this chapter you will discover how to tune the parameters of<br>\n", "machine learning algorithms in Python using the scikit-learn. After completing this lesson you<br>\n", "will know:<br>\n", "1. The importance of algorithm parameter tuning to improve algorithm performance.<br>\n", "2. How to use a grid search algorithm tuning strategy.<br>\n", "3. How to use a random search algorithm tuning strategy.<br>\n", "Let's get started.<br>\n", "**Machine Learning Algorithm Parameters**<br>\n", "Algorithm tuning is a \fnal step in the process of applied machine learning before \fnalizing your<br>\n", "model. It is sometimes called hyperparameter optimization where the algorithm parameters<br>\n", "are referred to as hyperparameters, whereas the coe\u000ecients found by the machine learning<br>\n", "algorithm itself are referred to as parameters. Optimization suggests the search-nature of the<br>\n", "problem. Phrased as a search problem, you can use di\u000berent search strategies to \fnd a good and<br>\n", "robust parameter or set of parameters for an algorithm on a given problem. Python scikit-learn<br>\n", "provides two simple methods for algorithm parameter tuning:<br>\n", "*   Grid Search Parameter Tuning.<br>\n", "*   Random Search Parameter Tuning.<br>\n", "# B1.Grid Search Parameter Tuning<br>\n", "Grid search is an approach to parameter tuning that will methodically build and evaluate a<br>\n", "model for each combination of algorithm parameters speci\ffied in a grid. You can perform a grid<br>\n", "search using the GridSearchCV class1. The example below evaluates di\u000bfferent alpha values for<br>\n", "the Ridge Regression algorithm on the standard diabetes dataset. This is a one-dimensional<br>\n", "grid search.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Grid Search for Algorithm Tuning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy\n", "from pandas import read_csv\n", "from sklearn.linear_model import Ridge\n", "from sklearn.model_selection import GridSearchCV\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n", "param_grid = dict(alpha=alphas)\n", "model = Ridge()\n", "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n", "grid.fit(X, Y)\n", "print(grid.best_score_)\n", "print(grid.best_estimator_.alpha)\n", "#Running the example lists out the optimal score achieved and the set of parameters in the\n", "# grid that achieved that score. In this case the alpha value of 1.0."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# B2.Random Search Parameter Tuning<br>\n", "Random search is an approach to parameter tuning that will sample algorithm parameters from<br>\n", "a random distribution (i.e. uniform) for a \fxed number of iterations. A model is constructed<br>\n", "and evaluated for each combination of parameters chosen. You can perform a random search<br>\n", "for algorithm parameters using the RandomizedSearchCV class2. The example below evaluates<br>\n", "di\u000berent random alpha values between 0 and 1 for the Ridge Regression algorithm on the<br>\n", "standard diabetes dataset. A total of 100 iterations are performed with uniformly random alpha<br>\n", "values selected in the range between 0 and 1 (the range that alpha values can take).<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Randomized for Algorithm Tuning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy\n", "from scipy.stats import uniform\n", "from sklearn.linear_model import Ridge\n", "from sklearn.model_selection import RandomizedSearchCV\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "param_grid = {'alpha': uniform()}\n", "model = Ridge()\n", "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100,\n", "random_state=7)\n", "rsearch.fit(X, Y)\n", "print(rsearch.best_score_)\n", "print(rsearch.best_estimator_.alpha)\n", "# Running the example produces results much like those in the grid search example above. An\n", "# optimal alpha value near 1.0 is discovered."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n**Summary**<br>\n", "Algorithm parameter tuning is an important step for improving algorithm performance right<br>\n", "before presenting results or preparing a system for production. In this chapter you discovered<br>\n", "algorithm parameter tuning and two methods that you can use right now in Python and<br>\n", "scikit-learn to improve your algorithm results:<br>\n", "\u0088 Grid Search Parameter Tuning<br>\n", "\u0088 Random Search Parameter Tuning<br>\n", "**Next**<br>\n", "This lesson concludes the coverage of techniques that you can use to improve the performance of<br>\n", "algorithms on your dataset. In the next and \fnal lesson you will discover how you can \fnalize<br>\n", "your model for using it on unseen data.<br>\n", "# Save and Load Machine Learning Models<br>\n", "Finding an accurate machine learning model is not the end of the project. In this chapter you<br>\n", "will discover how to save and load your machine learning model in Python using scikit-learn.<br>\n", "This allows you to save your model to \fle and load it later in order to make predictions. After<br>\n", "completing this lesson you will know:<br>\n", "1. The importance of serializing models for reuse.<br>\n", "2. How to use pickle to serialize and deserialize machine learning models.<br>\n", "3. How to use Joblib to serialize and deserialize machine learning models.<br>\n", "Let's get started.<br>\n", "# 1.Finalize Your Model with pickle<br>\n", "Pickle is the standard way of serializing objects in Python. You can use the pickle1 operation<br>\n", "to serialize your machine learning algorithms and save the serialized format to a \fle. Later you<br>\n", "can load this \fle to deserialize your model and use it to make new predictions. The example<br>\n", "below demonstrates how you can train a logistic regression model on the Pima Indians onset of<br>\n", "diabetes dataset, save the model to \fle and load it to make predictions on the unseen test set.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Save Model Using Pickle"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from pickle import dump\n", "from pickle import load\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n", "# Fit the model on 33%\n", "model = LogisticRegression()\n", "model.fit(X_train, Y_train)\n", "# save the model to disk\n", "filename = 'finalized_model.sav'\n", "dump(model, open(filename, 'wb'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["some time later..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["load the model from disk"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loaded_model = load(open(filename, 'rb'))\n", "result = loaded_model.score(X_test, Y_test)\n", "print(result)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Running the example saves the model to finalized model.sav in your local working<br>\n", "directory. Load the saved model and evaluating it provides an estimate of accuracy of the model<br>\n", "on unseen data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# 2.Finalize Your Model with Joblib<br>\n", "The Joblib2 library is part of the SciPy ecosystem and provides utilities for pipelining Python<br>\n", "jobs. It provides utilities for saving and loading Python objects that make use of NumPy data<br>\n", "structures, e\u000eciently3. This can be useful for some machine learning algorithms that require a<br>\n", "lot of parameters or store the entire dataset (e.g. k-Nearest Neighbors). The example below<br>\n", "demonstrates how you can train a logistic regression model on the Pima Indians onset of diabetes<br>\n", "dataset, save the model to \fle using Joblib and load it to make predictions on the unseen test<br>\n", "set.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Save Model Using joblib"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.externals.joblib import dump\n", "from sklearn.externals.joblib import load\n", "array = data_to_use.values\n", "X = array[:,0:9]\n", "Y = array[:,9]\n", "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit the model on 33%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = LogisticRegression()\n", "model.fit(X_train, Y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["save the model to disk"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["filename = 'finalized_model.sav'\n", "dump(model, filename)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["some time later..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["load the model from disk"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loaded_model = load(filename)\n", "result = loaded_model.score(X_test, Y_test)\n", "print(result)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Running the example saves the model to \fle as finalized model.sav and also creates one<br>\n", "\fle for each NumPy array in the model (four additional \fles). After the model is loaded an<br>\n", "estimate of the accuracy of the model on unseen data is reported."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n**Tips for Finalizing Your Model**<br>\n", "This section lists some important considerations when \fnalizing your machine learning models.<br>\n", "\u0088 **Python Version**. Take note of the Python version. You almost certainly require the<br>\n", "same major (and maybe minor) version of Python used to serialize the model when you<br>\n", "later load it and deserialize it.<br>\n", "\u0088 **Library Versions**. The version of all major libraries used in your machine learning<br>\n", "project almost certainly need to be the same when deserializing a saved model. This is<br>\n", "not limited to the version of NumPy and the version of scikit-learn.<br>\n", "\u0088 **Manual Serialization**. You might like to manually output the parameters of your<br>\n", "learned model so that you can use them directly in scikit-learn or another platform in<br>\n", "the future. Often the techniques used internally by machine learning algorithms to make<br>\n", "predictions are a lot simpler than those used to learn the parameters can may be easy to<br>\n", "implement in custom code that you have control over.<br>\n", "Take note of the version so that you can re-create the environment if for some reason you<br>\n", "cannot reload your model on another machine or another platform at a later time.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["values = data_to_use.values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Y = values[:,28]\n", "X = values[:,0:28]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nBefore we run our machine learning models, we need to set a random number to use to seed them. This can be any random number that you'd like it to be. Some people like to use a random number generator but for the purposes of this, I'll just set it to 12 (it could just as easily be 1 or 3 or 1023 or any other number).\n<br>\n", "random_seed = 12<br>\n", "ow we need to set up our models that we'll be testing out. We'll set up a list of the models and give them each a name. Additionally, I'm going to set up the blank arrays/lists for the outcomes and the names of the models to use for comparison.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["outcome = []\n", "model_names = []\n", "models = [('LogReg', LogisticRegression()), \n", "          ('SVM', SVC()), \n", "          ('DecTree', DecisionTreeClassifier()),\n", "          ('KNN', KNeighborsClassifier()),\n", "          ('LinDisc', LinearDiscriminantAnalysis()),\n", "          ('GaussianNB', GaussianNB())]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nWe are going to use a k-fold validation to evaluate each algorithm and will run through each model with a for loop, running the analysis and then storing the outcomes into the lists we created above. We'll use a 10-fold cross validation.\n<br>\n", "for model_name, model in models:<br>\n", "    k_fold_validation = model_selection.KFold(n_splits=10, random_state=random_seed)<br>\n", "    results = model_selection.cross_val_score(model, X, Y, cv=k_fold_validation, scoring='accuracy')<br>\n", "    outcome.append(results)<br>\n", "    model_names.append(model_name)<br>\n", "    output_message = \"%s| Mean=%f STD=%f\" % (model_name, results.mean(), results.std())<br>\n", "    print(output_message)<br>\n", "rom the above, it looks like the Logistic Regression, Support Vector Machine and Linear Discrimation Analysis methods are providing the best results. If we take a look at a box plot to see what the accuracy is for each cross validation fold, we can see just how good each does relative to each other and their means.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = plt.figure()\n", "fig.suptitle('Machine Learning Model Comparison')\n", "ax = fig.add_subplot(111)\n", "plt.boxplot(outcome)\n", "ax.set_xticklabels(model_names)\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}